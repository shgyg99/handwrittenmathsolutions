{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shgyg99/handwrittenmathsolutions/blob/main/mathTOlatex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hi7OmzqQj_Te"
      },
      "source": [
        "# **ðŸ”´ENVIRONMENT SETUP**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "G2TVFhIEk7ib"
      },
      "outputs": [],
      "source": [
        "!pip install -q torchmetrics\n",
        "!pip install -q wandb\n",
        "!pip install -q fastai\n",
        "!pip install -q pytorch_lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2MLu6bOFY6y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21069292-46b2-4759-f480-7181d4c1b2e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torchvision 0.20.0\n",
            "Uninstalling torchvision-0.20.0:\n",
            "  Successfully uninstalled torchvision-0.20.0\n",
            "Found existing installation: torch 2.5.0\n",
            "Uninstalling torch-2.5.0:\n",
            "  Successfully uninstalled torch-2.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall torchvision torch -y\n",
        "!pip install -q torch==2.5 torchvision==0.20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFEbeoVYlLRg"
      },
      "source": [
        "# **ðŸ”´IMPORT LIBS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWODQ4BLjblB"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import os\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms as TT\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from torch import optim\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
        "import wandb\n",
        "import tqdm\n",
        "import torchmetrics as tm\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5bNJaBDlhbl"
      },
      "source": [
        "# **ðŸ”´UTILS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HeLrwBOvlg5j"
      },
      "outputs": [],
      "source": [
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mj2ouMgDlmnr"
      },
      "outputs": [],
      "source": [
        "def num_trainable_params(model):\n",
        "  nums = sum(p.numel() for p in model.parameters() if p.requires_grad)/1e6\n",
        "  return nums"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQZtnlwflobp"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  if torch.cuda.is_available():\n",
        "      torch.cuda.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZJ2on11ltK-"
      },
      "source": [
        "# **ðŸ”´ARGUMENTS**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"seed\": 1234,\n",
        "\n",
        "    \"trainer\": {\n",
        "        \"overfit_batches\": 0.0,\n",
        "        \"check_val_every_n_epoch\": 3,\n",
        "        \"fast_dev_run\": False,\n",
        "        \"max_epochs\": 100,\n",
        "        \"min_epochs\": 1,\n",
        "        \"num_sanity_val_steps\": 0,\n",
        "    },\n",
        "\n",
        "    \"callbacks\": {\n",
        "        \"model_checkpoint\": {\n",
        "            \"save_top_k\": 1,\n",
        "            \"save_weights_only\": True,\n",
        "            \"mode\": \"min\",\n",
        "            \"monitor\": \"val/loss\",\n",
        "            \"filename\": \"{epoch}-{val/loss:.2f}-{val/cer:.2f}\"\n",
        "        },\n",
        "        \"early_stopping\": {\n",
        "            \"patience\": 3,\n",
        "            \"mode\": \"min\",\n",
        "            \"monitor\": \"val/loss\",\n",
        "            \"min_delta\": 0.001\n",
        "        }\n",
        "    },\n",
        "\n",
        "    \"data\": {\n",
        "        \"batch_size\": 8,\n",
        "        \"num_workers\": 0,\n",
        "        \"pin_memory\": False\n",
        "    },\n",
        "\n",
        "    \"lit_model\": {\n",
        "        # Optimizer\n",
        "        \"lr\": 0.00001,\n",
        "        \"weight_decay\": 0.0001,\n",
        "\n",
        "        # Scheduler\n",
        "        \"milestones\": [10],\n",
        "        \"gamma\": 0.5,\n",
        "\n",
        "        # Model\n",
        "        \"d_model\": 128,\n",
        "        \"dim_feedforward\": 256,\n",
        "        \"nhead\": 4,\n",
        "        \"dropout\": 0.1,\n",
        "        \"num_decoder_layers\": 3,\n",
        "        \"max_output_len\": 200\n",
        "    },\n",
        "\n",
        "    \"logger\": {\n",
        "        \"project\": \"image-to-latex\"\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "MmNE3wyWx4dw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqCs08IPlqLg"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/papersFolder'\n",
        "batch_size=8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_WJRepJQcuB"
      },
      "source": [
        "# **ðŸ”´CUSTOM DATASET**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokens = [\n",
        "#     '\\\\', '{', '}', '^', '+', '-', \"'\", '_', '!', '.', '/', '&', '%', '*',\n",
        "#     '\\\\frac', '\\\\times', '\\\\lim', '\\\\sin', '\\\\cos', '\\\\tan', '\\\\cot', '\\\\csc', '\\\\sec',\n",
        "#     '\\\\sqrt', '\\\\sum', '\\\\rightarrow', '\\\\Rightarrow', '\\\\leftarrow', '\\\\Leftarrow',\n",
        "#     '\\\\right', '\\\\left', '\\\\alpha', '\\\\beta', '|', '\\\\Delta', '\\\\delta', '\\\\gamma', '\\\\lambda',\n",
        "#     '\\\\min', '\\\\max', '(', ')', '<', '>', '=', '\\\\pm', '\\\\mp', '\\\\neq', '\\\\infty', '\\\\matrix',\n",
        "#     '[', ']', '\\\\in', '\\\\notin', '\\\\cap', '\\\\cup', '\\\\begin', '\\\\end', '\\\\dots', '\\\\int', ':',\n",
        "#     '\\\\ln', '\\\\pi', '\\\\theta', '\\\\to', '\\\\arctan', '\\\\arccot', '\\\\arcsin', '\\\\arccos',\n",
        "#     '\\\\log', '\\\\sinh', '\\\\cosh', '\\\\coth', '\\\\tanh', '\\\\degree', '\\\\dev', '\\\\sim',\n",
        "#     '\\\\forall', '\\\\emptyset', '\\\\buildrelF', '\\\\bar', '\\\\exists', '\\\\varepsilon', '\\\\partial',\n",
        "#     '\\\\hat', '\\\\triangle', '\\\\mathbb', '\\\\simeq',\n",
        "#     'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n",
        "#     'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
        "#     '0', '1', '2', '3', '4', '5', '6', '7', '8', '9'\n",
        "# ]\n",
        "\n",
        "# tokenizer = Tokenizer(models.BPE())\n",
        "\n",
        "# special_tokens = [\"<s>\", \"</s>\", \"<unk>\", \"<pad>\"] + tokens\n",
        "# trainer = trainers.BpeTrainer(vocab_size=len(special_tokens), special_tokens=special_tokens)\n",
        "\n",
        "# tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "\n",
        "# files = [\"/content/drive/MyDrive/papersFolder/latex_formulas.txt\"]\n",
        "# tokenizer.train(files, trainer)\n",
        "\n",
        "# # Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„\n",
        "# tokenizer.save(\"/content/drive/MyDrive/papersFolder/latex_tokenizer.json\")\n",
        "\n",
        "# print(\"âœ…Done\")"
      ],
      "metadata": {
        "id": "JUjWVndOluvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def target_transform(label):\n",
        "  tokenizer = Tokenizer.from_file(f\"{path}/latex_tokenizer.json\")\n",
        "  encoded_label = tokenizer.encode(f'<s>{label}</s>')\n",
        "  return torch.LongTensor(encoded_label.ids)"
      ],
      "metadata": {
        "id": "zvHArRPuhwts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_transform = TT.Compose([\n",
        "    TT.Grayscale(num_output_channels=1),\n",
        "    TT.Resize((256, 512)),\n",
        "    TT.ToTensor(),\n",
        "    TT.Normalize(mean=[0.5], std=[0.5])\n",
        "])"
      ],
      "metadata": {
        "id": "AxHJ3t4kiyCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bi1fMdFOQE-6"
      },
      "outputs": [],
      "source": [
        "class LatexImages(Dataset):\n",
        "  def __init__(self, path, image_transform=None, target_transform=None):\n",
        "    self.path = path\n",
        "    self.image_transform = image_transform\n",
        "    self.target_transform = target_transform\n",
        "\n",
        "    csv = pd.read_csv(os.path.join(self.path, 'merged_sorted.csv'))\n",
        "\n",
        "    self.image_paths = []\n",
        "    self.labels = []\n",
        "    for i in os.listdir(os.path.join(self.path, 'cropped')):\n",
        "        image_path = fr'.\\cropped\\{i}'\n",
        "        try:\n",
        "          label = csv[csv['Image Path'] == image_path]['LaTeX Label'].values[0]\n",
        "          self.image_paths.append(os.path.join(self.path, 'cropped', i))\n",
        "          self.labels.append(label)\n",
        "        except:\n",
        "          pass\n",
        "\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    image_path = self.image_paths[index]\n",
        "\n",
        "    if self.target_transform:\n",
        "      label = self.target_transform(self.labels[index])\n",
        "    else:\n",
        "      label = self.labels[index]\n",
        "\n",
        "    if self.image_transform:\n",
        "      image = self.image_transform(Image.open(image_path))\n",
        "    else:\n",
        "      image = Image.open(image_path)\n",
        "\n",
        "    return image, label\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.image_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHi-cTp4UIgd"
      },
      "outputs": [],
      "source": [
        "dataset = LatexImages(path=path, image_transform=image_transform, target_transform=target_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FsZ45t_EbGLe"
      },
      "outputs": [],
      "source": [
        "im, la = dataset.__getitem__(np.random.randint(0, dataset.__len__()))\n",
        "\n",
        "plt.imshow(TT.ToPILImage()(im))\n",
        "print(im.shape, la.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ðŸ”´Data Loader**"
      ],
      "metadata": {
        "id": "Tq2Ryx5Dk8km"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, test_set = random_split(dataset, [int(len(dataset)*0.9), len(dataset)-int(len(dataset)*0.9)])"
      ],
      "metadata": {
        "id": "1svWDEsXmkse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(data):\n",
        "    token = Tokenizer.from_file(f\"{path}/latex_tokenizer.json\")\n",
        "    pad_value = token.get_vocab().get('<pad>', 3)\n",
        "    tensors, targets = zip(*data)\n",
        "    targets = [torch.tensor(t, dtype=torch.long) for t in targets]\n",
        "    features = pad_sequence(targets, padding_value=pad_value, batch_first=True)\n",
        "    try:\n",
        "        tensors = torch.stack(tensors)\n",
        "    except RuntimeError as e:\n",
        "        print(f\"stack error: {e}\")\n",
        "        for i, t in enumerate(tensors):\n",
        "            print(f\"tensor shape error {i}: {t.shape}\")\n",
        "\n",
        "    return tensors, features"
      ],
      "metadata": {
        "id": "QALwTzOJgyE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, pin_memory=True, num_workers=4)\n",
        "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, pin_memory=True)"
      ],
      "metadata": {
        "id": "ces-ckAimjmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = next(iter(train_loader))\n",
        "x.shape, y.shape"
      ],
      "metadata": {
        "id": "ggBhGWhYnnBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ðŸ”´Metric**"
      ],
      "metadata": {
        "id": "0Pifowx-rjej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "id": "YMd_SbG0syuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import editdistance\n",
        "from typing import Set\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torchmetrics import Metric\n",
        "\n",
        "\n",
        "class CharacterErrorRate(Metric):\n",
        "    def __init__(self, ignore_indices: Set[int], *args):\n",
        "        super().__init__(*args)\n",
        "        self.ignore_indices = ignore_indices\n",
        "        self.add_state(\"error\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
        "        self.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
        "        self.error: Tensor\n",
        "        self.total: Tensor\n",
        "\n",
        "    def update(self, preds, targets):\n",
        "        N = preds.shape[0]\n",
        "        for i in range(N):\n",
        "            pred = [token for token in preds[i].tolist() if token not in self.ignore_indices]\n",
        "            target = [token for token in targets[i].tolist() if token not in self.ignore_indices]\n",
        "            distance = editdistance.distance(pred, target)\n",
        "            if max(len(pred), len(target)) > 0:\n",
        "                self.error += distance / max(len(pred), len(target))\n",
        "        self.total += N\n",
        "\n",
        "    def compute(self) -> Tensor:\n",
        "        return self.error / self.total"
      ],
      "metadata": {
        "id": "OgRow6K_rpFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ðŸ”´Functions**"
      ],
      "metadata": {
        "id": "7WDNaXpjnB3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from typing import List\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from pytorch_lightning import LightningModule\n",
        "\n",
        "\n",
        "class LitResNetTransformer(LightningModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        dim_feedforward: int,\n",
        "        nhead: int,\n",
        "        dropout: float,\n",
        "        num_decoder_layers: int,\n",
        "        max_output_len: int,\n",
        "        lr: float = 0.001,\n",
        "        weight_decay: float = 0.0001,\n",
        "        milestones: List[int] = [5],\n",
        "        gamma: float = 0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.lr = lr\n",
        "        self.weight_decay = weight_decay\n",
        "        self.milestones = milestones\n",
        "        self.gamma = gamma\n",
        "\n",
        "        self.tokenizer = Tokenizer.from_file(f\"{path}/latex_tokenizer.json\")\n",
        "        self.model = ResNetTransformer(\n",
        "            d_model=d_model,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            nhead=nhead,\n",
        "            dropout=dropout,\n",
        "            num_decoder_layers=num_decoder_layers,\n",
        "            max_output_len=max_output_len,\n",
        "            sos_index=self.tokenizer.get_vocab()['<s>'],\n",
        "            eos_index=self.tokenizer.get_vocab()['</s>'],\n",
        "            pad_index=self.tokenizer.get_vocab()['<pad>'],\n",
        "            num_classes=self.tokenizer.get_vocab_size(),\n",
        "        )\n",
        "        self.loss_fn = nn.CrossEntropyLoss(ignore_index=self.tokenizer.get_vocab()['<pad>'])\n",
        "        self.val_cer = CharacterErrorRate({self.tokenizer.get_vocab()['<pad>'], self.tokenizer.get_vocab()['<s>'], self.tokenizer.get_vocab()['</s>']})\n",
        "        self.test_cer = CharacterErrorRate({self.tokenizer.get_vocab()['<pad>'], self.tokenizer.get_vocab()['<s>'], self.tokenizer.get_vocab()['</s>']})\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        imgs, targets = batch\n",
        "        logits = self.model(imgs, targets[:, :-1])\n",
        "        loss = self.loss_fn(logits, targets[:, 1:])\n",
        "        self.log(\"train/loss\", loss)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        imgs, targets = batch\n",
        "        logits = self.model(imgs, targets[:, :-1])\n",
        "        loss = self.loss_fn(logits, targets[:, 1:])\n",
        "        self.log(\"val/loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
        "\n",
        "        preds = self.model.predict(imgs)\n",
        "        val_cer = self.val_cer(preds, targets)\n",
        "        self.log(\"val/cer\", val_cer)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        imgs, targets = batch\n",
        "        preds = self.model.predict(imgs)\n",
        "        test_cer = self.test_cer(preds, targets)\n",
        "        self.log(\"test/cer\", test_cer)\n",
        "\n",
        "        self.test_step_outputs.append(preds)\n",
        "        return preds\n",
        "\n",
        "    def on_test_epoch_end(self):\n",
        "        with open(f\"{path}/test_predictions.txt\", \"w\") as f:\n",
        "            for preds in self.test_step_outputs:\n",
        "                for pred in preds:\n",
        "                    decoded = self.tokenizer.decode(pred.tolist())\n",
        "                    decoded.append(\"\\n\")\n",
        "                    decoded_str = \" \".join(decoded)\n",
        "                    f.write(decoded_str)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
        "        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=self.milestones, gamma=self.gamma)\n",
        "        return [optimizer], [scheduler]"
      ],
      "metadata": {
        "id": "fl-5ILMmra1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ðŸ”´Model**"
      ],
      "metadata": {
        "id": "qQrcAWNgnVt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "\n",
        "\n",
        "class PositionalEncoding2D(nn.Module):\n",
        "    \"\"\"2-D positional encodings for the feature maps produced by the encoder.\n",
        "\n",
        "    Following https://arxiv.org/abs/2103.06450 by Sumeet Singh.\n",
        "\n",
        "    Reference:\n",
        "    https://github.com/full-stack-deep-learning/fsdl-text-recognizer-2021-labs/blob/main/lab9/text_recognizer/models/transformer_util.py\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model: int, max_h: int = 2000, max_w: int = 2000) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        assert d_model % 2 == 0, f\"Embedding depth {d_model} is not even\"\n",
        "        pe = self.make_pe(d_model, max_h, max_w)  # (d_model, max_h, max_w)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    @staticmethod\n",
        "    def make_pe(d_model: int, max_h: int, max_w: int) -> Tensor:\n",
        "        \"\"\"Compute positional encoding.\"\"\"\n",
        "        pe_h = PositionalEncoding1D.make_pe(d_model=d_model // 2, max_len=max_h)  # (max_h, 1 d_model // 2)\n",
        "        pe_h = pe_h.permute(2, 0, 1).expand(-1, -1, max_w)  # (d_model // 2, max_h, max_w)\n",
        "\n",
        "        pe_w = PositionalEncoding1D.make_pe(d_model=d_model // 2, max_len=max_w)  # (max_w, 1, d_model // 2)\n",
        "        pe_w = pe_w.permute(2, 1, 0).expand(-1, max_h, -1)  # (d_model // 2, max_h, max_w)\n",
        "\n",
        "        pe = torch.cat([pe_h, pe_w], dim=0)  # (d_model, max_h, max_w)\n",
        "        return pe\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"Forward pass.\n",
        "\n",
        "        Args:\n",
        "            x: (B, d_model, H, W)\n",
        "\n",
        "        Returns:\n",
        "            (B, d_model, H, W)\n",
        "        \"\"\"\n",
        "        assert x.shape[1] == self.pe.shape[0]  # type: ignore\n",
        "        x = x + self.pe[:, : x.size(2), : x.size(3)]  # type: ignore\n",
        "        return x\n",
        "\n",
        "\n",
        "class PositionalEncoding1D(nn.Module):\n",
        "    \"\"\"Classic Attention-is-all-you-need positional encoding.\"\"\"\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000) -> None:\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        pe = self.make_pe(d_model, max_len)  # (max_len, 1, d_model)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    @staticmethod\n",
        "    def make_pe(d_model: int, max_len: int) -> Tensor:\n",
        "        \"\"\"Compute positional encoding.\"\"\"\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(1)\n",
        "        return pe\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"Forward pass.\n",
        "\n",
        "        Args:\n",
        "            x: (S, B, d_model)\n",
        "\n",
        "        Returns:\n",
        "            (B, d_model, H, W)\n",
        "        \"\"\"\n",
        "        assert x.shape[2] == self.pe.shape[2]  # type: ignore\n",
        "        x = x + self.pe[: x.size(0)]  # type: ignore\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "QhxtHgqFowZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Union\n",
        "\n",
        "class ResNetTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        dim_feedforward: int,\n",
        "        nhead: int,\n",
        "        dropout: float,\n",
        "        num_decoder_layers: int,\n",
        "        max_output_len: int,\n",
        "        sos_index: int,\n",
        "        eos_index: int,\n",
        "        pad_index: int,\n",
        "        num_classes: int,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.max_output_len = max_output_len + 2\n",
        "        self.sos_index = sos_index\n",
        "        self.eos_index = eos_index\n",
        "        self.pad_index = pad_index\n",
        "\n",
        "        # Encoder\n",
        "        resnet = torchvision.models.resnet18(pretrained=False)\n",
        "        self.backbone = nn.Sequential(\n",
        "            resnet.conv1,\n",
        "            resnet.bn1,\n",
        "            resnet.relu,\n",
        "            resnet.maxpool,\n",
        "            resnet.layer1,\n",
        "            resnet.layer2,\n",
        "            resnet.layer3,\n",
        "        )\n",
        "        self.bottleneck = nn.Conv2d(256, self.d_model, 1)\n",
        "        self.image_positional_encoder = PositionalEncoding2D(self.d_model)\n",
        "\n",
        "        # Decoder\n",
        "        self.embedding = nn.Embedding(num_classes, self.d_model)\n",
        "        self.y_mask = generate_square_subsequent_mask(self.max_output_len)\n",
        "        self.word_positional_encoder = PositionalEncoding1D(self.d_model, max_len=self.max_output_len)\n",
        "        transformer_decoder_layer = nn.TransformerDecoderLayer(self.d_model, nhead, dim_feedforward, dropout)\n",
        "        self.transformer_decoder = nn.TransformerDecoder(transformer_decoder_layer, num_decoder_layers)\n",
        "        self.fc = nn.Linear(self.d_model, num_classes)\n",
        "\n",
        "        # It is empirically important to initialize weights properly\n",
        "        if self.training:\n",
        "            self._init_weights()\n",
        "\n",
        "    def _init_weights(self) -> None:\n",
        "        \"\"\"Initialize weights.\"\"\"\n",
        "        init_range = 0.1\n",
        "        self.embedding.weight.data.uniform_(-init_range, init_range)\n",
        "        self.fc.bias.data.zero_()\n",
        "        self.fc.weight.data.uniform_(-init_range, init_range)\n",
        "\n",
        "        nn.init.kaiming_normal_(\n",
        "            self.bottleneck.weight.data,\n",
        "            a=0,\n",
        "            mode=\"fan_out\",\n",
        "            nonlinearity=\"relu\",\n",
        "        )\n",
        "        if self.bottleneck.bias is not None:\n",
        "            _, fan_out = nn.init._calculate_fan_in_and_fan_out(self.bottleneck.weight.data)\n",
        "            bound = 1 / math.sqrt(fan_out)\n",
        "            nn.init.normal_(self.bottleneck.bias, -bound, bound)\n",
        "\n",
        "    def forward(self, x: Tensor, y: Tensor) -> Tensor:\n",
        "        \"\"\"Forward pass.\n",
        "\n",
        "        Args:\n",
        "            x: (B, _E, _H, _W)\n",
        "            y: (B, Sy) with elements in (0, num_classes - 1)\n",
        "\n",
        "        Returns:\n",
        "            (B, num_classes, Sy) logits\n",
        "        \"\"\"\n",
        "        encoded_x = self.encode(x)  # (Sx, B, E)\n",
        "        output = self.decode(y, encoded_x)  # (Sy, B, num_classes)\n",
        "        output = output.permute(1, 2, 0)  # (B, num_classes, Sy)\n",
        "        return output\n",
        "\n",
        "    def encode(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"Encode inputs.\n",
        "\n",
        "        Args:\n",
        "            x: (B, C, _H, _W)\n",
        "\n",
        "        Returns:\n",
        "            (Sx, B, E)\n",
        "        \"\"\"\n",
        "        # Resnet expects 3 channels but training images are in gray scale\n",
        "        if x.shape[1] == 1:\n",
        "            x = x.repeat(1, 3, 1, 1)\n",
        "        x = self.backbone(x)  # (B, RESNET_DIM, H, W); H = _H // 32, W = _W // 32\n",
        "        x = self.bottleneck(x)  # (B, E, H, W)\n",
        "        x = self.image_positional_encoder(x)  # (B, E, H, W)\n",
        "        x = x.flatten(start_dim=2)  # (B, E, H * W)\n",
        "        x = x.permute(2, 0, 1)  # (Sx, B, E); Sx = H * W\n",
        "        return x\n",
        "\n",
        "    def decode(self, y: Tensor, encoded_x: Tensor) -> Tensor:\n",
        "        \"\"\"Decode encoded inputs with teacher-forcing.\n",
        "\n",
        "        Args:\n",
        "            encoded_x: (Sx, B, E)\n",
        "            y: (B, Sy) with elements in (0, num_classes - 1)\n",
        "\n",
        "        Returns:\n",
        "            (Sy, B, num_classes) logits\n",
        "        \"\"\"\n",
        "        y = y.permute(1, 0)  # (Sy, B)\n",
        "        y = self.embedding(y) * math.sqrt(self.d_model)  # (Sy, B, E)\n",
        "        y = self.word_positional_encoder(y)  # (Sy, B, E)\n",
        "        Sy = y.shape[0]\n",
        "        y_mask = self.y_mask[:Sy, :Sy].type_as(encoded_x)  # (Sy, Sy)\n",
        "        output = self.transformer_decoder(y, encoded_x, y_mask)  # (Sy, B, E)\n",
        "        output = self.fc(output)  # (Sy, B, num_classes)\n",
        "        return output\n",
        "\n",
        "    def predict(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"Make predctions at inference time.\n",
        "\n",
        "        Args:\n",
        "            x: (B, C, H, W). Input images.\n",
        "\n",
        "        Returns:\n",
        "            (B, max_output_len) with elements in (0, num_classes - 1).\n",
        "        \"\"\"\n",
        "        B = x.shape[0]\n",
        "        S = self.max_output_len\n",
        "\n",
        "        encoded_x = self.encode(x)  # (Sx, B, E)\n",
        "\n",
        "        output_indices = torch.full((B, S), self.pad_index).type_as(x).long()\n",
        "        output_indices[:, 0] = self.sos_index\n",
        "        has_ended = torch.full((B,), False)\n",
        "\n",
        "        for Sy in range(1, S):\n",
        "            y = output_indices[:, :Sy]  # (B, Sy)\n",
        "            logits = self.decode(y, encoded_x)  # (Sy, B, num_classes)\n",
        "            # Select the token with the highest conditional probability\n",
        "            output = torch.argmax(logits, dim=-1)  # (Sy, B)\n",
        "            output_indices[:, Sy] = output[-1:]  # Set the last output token\n",
        "\n",
        "            # Early stopping of prediction loop to speed up prediction\n",
        "            has_ended |= (output_indices[:, Sy] == self.eos_index).type_as(has_ended)\n",
        "            if torch.all(has_ended):\n",
        "                break\n",
        "\n",
        "        # Set all tokens after end token to be padding\n",
        "        eos_positions = find_first(output_indices, self.eos_index)\n",
        "        for i in range(B):\n",
        "            j = int(eos_positions[i].item()) + 1\n",
        "            output_indices[i, j:] = self.pad_index\n",
        "\n",
        "        return output_indices\n",
        "\n",
        "\n",
        "def generate_square_subsequent_mask(size: int) -> Tensor:\n",
        "    \"\"\"Generate a triangular (size, size) mask.\"\"\"\n",
        "    mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float(\"-inf\")).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "\n",
        "def find_first(x: Tensor, element: Union[int, float], dim: int = 1) -> Tensor:\n",
        "    \"\"\"Find the first occurence of element in x along a given dimension.\n",
        "\n",
        "    Args:\n",
        "        x: The input tensor to be searched.\n",
        "        element: The number to look for.\n",
        "        dim: The dimension to reduce.\n",
        "\n",
        "    Returns:\n",
        "        Indices of the first occurence of the element in x. If not found, return the\n",
        "        length of x along dim.\n",
        "\n",
        "    Usage:\n",
        "        >>> first_element(Tensor([[1, 2, 3], [2, 3, 3], [1, 1, 1]]), 3)\n",
        "        tensor([2, 1, 3])\n",
        "\n",
        "    Reference:\n",
        "        https://discuss.pytorch.org/t/first-nonzero-index/24769/9\n",
        "\n",
        "        I fixed an edge case where the element we are looking for is at index 0. The\n",
        "        original algorithm will return the length of x instead of 0.\n",
        "    \"\"\"\n",
        "    mask = x == element\n",
        "    found, indices = ((mask.cumsum(dim) == 1) & mask).max(dim)\n",
        "    indices[(~found) & (indices == 0)] = x.shape[dim]\n",
        "    return indices"
      ],
      "metadata": {
        "id": "M2x0z6CfndBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ðŸ”´Test**"
      ],
      "metadata": {
        "id": "p1Bc6eSSvPTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "id": "BF7O9jDYGfhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from argparse import Namespace\n",
        "from typing import List, Optional\n",
        "\n",
        "# import hydra\n",
        "# from omegaconf import DictConfig\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
        "from pytorch_lightning.loggers.wandb import WandbLogger\n",
        "\n",
        "lit_model = LitResNetTransformer(**config['lit_model']).to(device)\n",
        "\n",
        "callbacks: List[Callback] = []\n",
        "callbacks.append(ModelCheckpoint(**config['callbacks']['model_checkpoint']))\n",
        "callbacks.append(EarlyStopping(**config['callbacks']['early_stopping']))\n",
        "\n",
        "logger: Optional[WandbLogger] = None\n",
        "if config['logger']:\n",
        "  logger = WandbLogger(**config['logger'])\n",
        "\n",
        "trainer = Trainer(**config['trainer'], callbacks=callbacks, logger=logger)\n",
        "\n",
        "if trainer.logger:\n",
        "    trainer.logger.log_hyperparams(Namespace(**config))\n",
        "\n",
        "trainer.fit(lit_model, train_dataloaders=train_loader, val_dataloaders=test_loader)\n",
        "trainer.test(lit_model, dataloaders=test_loader)"
      ],
      "metadata": {
        "id": "FtKkBzkBuoUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UA2-Mo2w1tWV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1yMRQK3G9aUwuxik-DWirWTNisHq_QcsN",
      "authorship_tag": "ABX9TyMlKyKsk34NecEGj7TPRd7e",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}